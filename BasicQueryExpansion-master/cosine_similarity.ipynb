{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import re # for regular expressions\n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "import string\n",
    "import nltk # for text manipulation\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm import tqdm\n",
    "#from gensim.models.deprecated.doc2vec import LabeledSentence\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats \n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import common\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentList = pd.read_csv(\"Copy of Coronavirus Tweets - Copy.csv\", encoding='latin', usecols=['ID', 'Description',\n",
    "                                                                                                   'Sentiment',\n",
    "                                                                                                   'IsRelevant',\n",
    "                                                                                                   'tfVector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantweets(text):\n",
    "    text=str(text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z#]+', ' ',text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentList['Description']=DocumentList['Description'].apply(lambda x:cleantweets(x))\n",
    "#remove short words\n",
    "DocumentList['Description']=DocumentList['Description'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk pakage\n",
    "tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                           strip_handles=True,\n",
    "                           reduce_len=True)\n",
    "\n",
    "DocumentList['Des_token']=DocumentList['Description'].apply(lambda x:tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    token = []\n",
    "    for word in CountVectorizer().build_tokenizer()(sentence):\n",
    "        token.append(word)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentList['Des_token']=DocumentList['Description'].apply(lambda x:tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "def cleanstopword(tweet_tokens):\n",
    "  tweets_clean = []\n",
    "  for word in tweet_tokens:\n",
    "    if (word not in stopwords_english and word not in string.punctuation):\n",
    "      tweets_clean.append(''.join(word))\n",
    "  return tweets_clean\n",
    "##########################################3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentList['Des_token']=DocumentList['Des_token'].apply(lambda x:cleanstopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllTerms(textList):\n",
    "    terms = []\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            terms.append(textList[i][j])\n",
    "    return sorted(set(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = getAllTerms(DocumentList['Des_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "  \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(terms, vector_size=300, min_count=2,window=5, sg=1,workers=4)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x20a6c5b15f8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_w2v(doc_tokens):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model1.wv.index_to_key:\n",
    "                embeddings.append(model1.wv.word_vec(tok))\n",
    "            else:\n",
    "                embeddings.append(np.random.rand(300))\n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=['covid','vaccine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=get_embedding_w2v(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53551828, 0.41854745, 0.81609005, 0.57266972, 0.50472997,\n",
       "       0.41348212, 0.53923726, 0.80465376, 0.40086566, 0.07424934,\n",
       "       0.56985353, 0.24252448, 0.29681483, 0.35238451, 0.61461851,\n",
       "       0.8376321 , 0.36393108, 0.35544147, 0.54156872, 0.66569567,\n",
       "       0.58098772, 0.23273798, 0.13667955, 0.38882995, 0.41564568,\n",
       "       0.42368113, 0.71667582, 0.47430166, 0.58305173, 0.67677658,\n",
       "       0.64775851, 0.78045972, 0.60619613, 0.64312217, 0.8680184 ,\n",
       "       0.53780659, 0.40217122, 0.5234382 , 0.39456944, 0.33067418,\n",
       "       0.29526498, 0.21867255, 0.5303914 , 0.34551042, 0.17257685,\n",
       "       0.44004578, 0.5729511 , 0.49387241, 0.3585988 , 0.07662811,\n",
       "       0.37232853, 0.26929815, 0.38815603, 0.29538079, 0.29766316,\n",
       "       0.24519978, 0.69674651, 0.40886687, 0.36303954, 0.06569451,\n",
       "       0.49796969, 0.22557687, 0.63701466, 0.57208601, 0.68632729,\n",
       "       0.19848978, 0.37457135, 0.50427379, 0.67631295, 0.51261446,\n",
       "       0.9890376 , 0.6837944 , 0.36745047, 0.3048796 , 0.37300016,\n",
       "       0.56625283, 0.36807757, 0.2438126 , 0.35829185, 0.64771109,\n",
       "       0.608726  , 0.30207913, 0.24621921, 0.71222094, 0.58922372,\n",
       "       0.67456222, 0.48061322, 0.51593691, 0.47822687, 0.28186721,\n",
       "       0.52587217, 0.70276645, 0.29105322, 0.47339912, 0.09056816,\n",
       "       0.61221548, 0.40579748, 0.72074839, 0.40842272, 0.68177028,\n",
       "       0.55522404, 0.68662406, 0.54240988, 0.79545274, 0.63493804,\n",
       "       0.72354177, 0.33550756, 0.69991567, 0.54019716, 0.85303751,\n",
       "       0.31815913, 0.75768784, 0.35988891, 0.75702123, 0.31956208,\n",
       "       0.71139558, 0.55549322, 0.13763427, 0.66372265, 0.54779096,\n",
       "       0.38797963, 0.98922028, 0.4222182 , 0.66726421, 0.83065495,\n",
       "       0.26570754, 0.82745414, 0.68996487, 0.60043612, 0.7791594 ,\n",
       "       0.39047568, 0.36477892, 0.18889737, 0.32690252, 0.78339935,\n",
       "       0.81783641, 0.63096597, 0.30046112, 0.6931149 , 0.35210451,\n",
       "       0.19706418, 0.41834838, 0.46451097, 0.16850897, 0.63623282,\n",
       "       0.36024586, 0.46031812, 0.37341979, 0.14414573, 0.50439622,\n",
       "       0.64841813, 0.48346261, 0.23009448, 0.15887779, 0.4891018 ,\n",
       "       0.45322523, 0.83223945, 0.79359141, 0.41532094, 0.1592275 ,\n",
       "       0.1492909 , 0.52475972, 0.32703957, 0.42063548, 0.53299356,\n",
       "       0.53803596, 0.41351067, 0.65017012, 0.24313709, 0.63730686,\n",
       "       0.51681105, 0.45199506, 0.88757739, 0.5461539 , 0.58344817,\n",
       "       0.1446823 , 0.74745764, 0.44546062, 0.32752861, 0.63203655,\n",
       "       0.65139331, 0.34122927, 0.2528833 , 0.52454917, 0.49043708,\n",
       "       0.46915364, 0.5362985 , 0.72066556, 0.55590766, 0.72085739,\n",
       "       0.37869558, 0.57587203, 0.77779091, 0.80344277, 0.24781194,\n",
       "       0.39530364, 0.67254013, 0.66292769, 0.17554983, 0.95360883,\n",
       "       0.45378571, 0.40183921, 0.48294216, 0.16931701, 0.22854891,\n",
       "       0.84754483, 0.58083015, 0.13959767, 0.59660631, 0.21721592,\n",
       "       0.71015256, 0.09634521, 0.69221086, 0.51984076, 0.23740364,\n",
       "       0.476347  , 0.34643133, 0.23975926, 0.4637291 , 0.01864254,\n",
       "       0.44374695, 0.65068025, 0.3902204 , 0.35911598, 0.35741022,\n",
       "       0.44533122, 0.65834783, 0.37016493, 0.6907673 , 0.57559047,\n",
       "       0.1854523 , 0.99838578, 0.39379976, 0.36599091, 0.79538005,\n",
       "       0.64537743, 0.22695248, 0.54294902, 0.36660204, 0.33234029,\n",
       "       0.31213675, 0.78981377, 0.80349701, 0.52974641, 0.2606602 ,\n",
       "       0.35380372, 0.79932016, 0.09954938, 0.69096516, 0.359633  ,\n",
       "       0.55127195, 0.13031176, 0.09704451, 0.48166665, 0.48144914,\n",
       "       0.73273208, 0.5068172 , 0.56703047, 0.59450083, 0.09239769,\n",
       "       0.15093382, 0.42674645, 0.72372125, 0.87702719, 0.4321158 ,\n",
       "       0.50443504, 0.49551362, 0.88155615, 0.72822816, 0.21397345,\n",
       "       0.55933061, 0.62698385, 0.34304065, 0.63115333, 0.17968236,\n",
       "       0.31270215, 0.11836444, 0.06088099, 0.17653121, 0.4518964 ,\n",
       "       0.75022878, 0.01830219, 0.52085941, 0.2259207 , 0.24359034,\n",
       "       0.4610761 , 0.41626784, 0.22145866, 0.89082678, 0.51944428,\n",
       "       0.3074363 , 0.75991407, 0.84820949, 0.61147837, 0.09499773,\n",
       "       0.85987615, 0.52050143, 0.28310825, 0.6937547 , 0.61562754])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentList['vector']=DocumentList['Des_token'].apply(lambda x:get_embedding_w2v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>IsRelevant</th>\n",
       "      <th>ID</th>\n",
       "      <th>tfVector</th>\n",
       "      <th>Des_token</th>\n",
       "      <th>vector</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>too work retail the store where would get orders from the drivers know have told them work store near there store have been told day day basis roughly item</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>27369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[work, retail, store, would, get, orders, drivers, know, told, work, store, near, store, told, day, day, basis, roughly, item]</td>\n",
       "      <td>[0.5124790327156227, 0.4361002119303823, 0.598646048878666, 0.5733125012590152, 0.6037498071939685, 0.5274970532365403, 0.4721870527564337, 0.5475916829654918, 0.5308112906366967, 0.48899337409897...</td>\n",
       "      <td>0.918534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day covid diary all schools are shut friday meanwhile braved the supermarket armed with carrier bags and hand sanitizer the shelves where pretty empty also saw elderly gent wearing welding mask wh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>5035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[day, covid, diary, schools, shut, friday, meanwhile, braved, supermarket, armed, carrier, bags, hand, sanitizer, shelves, pretty, empty, also, saw, elderly, gent, wearing, welding, mask, bizarre]</td>\n",
       "      <td>[0.5704910972802842, 0.42575185715042246, 0.4029838927139648, 0.4468534234028975, 0.5703589785802079, 0.4997081231912942, 0.5690486378334831, 0.5628777879607043, 0.5074086862907985, 0.479480671662...</td>\n",
       "      <td>0.918260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>panic buying reaction coronavirus has meant drop donations for some foodbanks while some are also struggling buy supplies from grocery stores feeding america establishes covid response fund help f...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[panic, buying, reaction, coronavirus, meant, drop, donations, foodbanks, also, struggling, buy, supplies, grocery, stores, feeding, america, establishes, covid, response, fund, help, food, banks,...</td>\n",
       "      <td>[0.40902034422222444, 0.48297367409893754, 0.5720102381228351, 0.5024366942754132, 0.5595574700839206, 0.48308097911771897, 0.5265511825441006, 0.5386432218716487, 0.5678654419920186, 0.4369796631...</td>\n",
       "      <td>0.918055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can help but wonder the one third people saw the grocery store wearing masks are being forced use them after covid patients cough their face too</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>34612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[help, wonder, one, third, people, saw, grocery, store, wearing, masks, forced, use, covid, patients, cough, face]</td>\n",
       "      <td>[0.5469317265237348, 0.43453320213120455, 0.5551190337066506, 0.5392189638575862, 0.4937792399801246, 0.47348353336298626, 0.6429286203435995, 0.47986935553837523, 0.48194369694910566, 0.479314593...</td>\n",
       "      <td>0.917843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these unprecedented times calls unprecedented measures caf india appeals you support reach out the vulnerable people with urgent food supplies hygiene kits and accurate information about preventio...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>15272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[unprecedented, times, calls, unprecedented, measures, caf, india, appeals, support, reach, vulnerable, people, urgent, food, supplies, hygiene, kits, accurate, information, prevention, covid, ave...</td>\n",
       "      <td>[0.5291587559496079, 0.5223125262482532, 0.5753982354647033, 0.5025361869393413, 0.560602881778188, 0.5467917188106997, 0.5087006598894713, 0.45608143839640897, 0.43031883293045964, 0.445982860158...</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>come this the white house now advising everyone not head the grocery store pharmacy the coming two weeks the next two weeks are extraordinarily important white house response coordinator deborah b...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>29317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[come, white, house, advising, everyone, head, grocery, store, pharmacy, coming, two, weeks, next, two, weeks, extraordinarily, important, white, house, response, coordinator, deborah, birx, said]</td>\n",
       "      <td>[0.47007189003721184, 0.4204719778934094, 0.5015609020480467, 0.4885943592785753, 0.44131183004226, 0.5371532611159601, 0.49100569516295794, 0.5622903665518074, 0.4844938677326825, 0.5696832431299...</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>japan got great food mascots amp check out this one spotted month back haneda amp perhaps worth buying you worried about hygiene amp shortages the age</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>16968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[japan, got, great, food, mascots, amp, check, one, spotted, month, back, haneda, amp, perhaps, worth, buying, worried, hygiene, amp, shortages, age]</td>\n",
       "      <td>[0.545791345311021, 0.5603386347505851, 0.45267016461493637, 0.5254609942445622, 0.4983894252846796, 0.46931479714409174, 0.4051462448146384, 0.6386083200416551, 0.4442441333783032, 0.333136731118...</td>\n",
       "      <td>0.917388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earlier announced they are closing select doors now store closures inevitable most non essential retail will close luxury players amp the well capitalized should survive everyone else start your b...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[earlier, announced, closing, select, doors, store, closures, inevitable, non, essential, retail, close, luxury, players, amp, well, capitalized, survive, everyone, else, start, business, continui...</td>\n",
       "      <td>[0.45208328296077926, 0.5508184592911856, 0.5642579273214066, 0.5295067214475077, 0.5244205173411708, 0.5065622321868268, 0.5324084205359669, 0.5106640908545929, 0.4256287116742515, 0.400963577206...</td>\n",
       "      <td>0.917224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tip wash your hands with soap for seconds and use greater than alcohol based hand sanitizer whenever you return home from any activity that involves locations where other people have been</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>21381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tip, wash, hands, soap, seconds, use, greater, alcohol, based, hand, sanitizer, whenever, return, home, activity, involves, locations, people]</td>\n",
       "      <td>[0.5118022156369622, 0.561365798304358, 0.5359597525348093, 0.5809599342008541, 0.4202774099598013, 0.3551222709885664, 0.46811622067125264, 0.5567428197801534, 0.5660807258123945, 0.3573019305186...</td>\n",
       "      <td>0.917160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>interesting discovery consumer grade thermal tech won work for medical use cases such detecting covid fevers this the one pro camera action body temp way too low</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>7333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[interesting, discovery, consumer, grade, thermal, tech, work, medical, use, cases, detecting, covid, fevers, one, pro, camera, action, body, temp, way, low]</td>\n",
       "      <td>[0.5463184304658134, 0.48810772070873637, 0.6176658352086668, 0.5096231623674524, 0.5542031067367708, 0.4519572350732188, 0.3272063781075404, 0.5116562078223029, 0.49214538979283373, 0.57832158894...</td>\n",
       "      <td>0.916869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                               Description  \\\n",
       "0                                              too work retail the store where would get orders from the drivers know have told them work store near there store have been told day day basis roughly item   \n",
       "1  day covid diary all schools are shut friday meanwhile braved the supermarket armed with carrier bags and hand sanitizer the shelves where pretty empty also saw elderly gent wearing welding mask wh...   \n",
       "2  panic buying reaction coronavirus has meant drop donations for some foodbanks while some are also struggling buy supplies from grocery stores feeding america establishes covid response fund help f...   \n",
       "3                                                         can help but wonder the one third people saw the grocery store wearing masks are being forced use them after covid patients cough their face too   \n",
       "4  these unprecedented times calls unprecedented measures caf india appeals you support reach out the vulnerable people with urgent food supplies hygiene kits and accurate information about preventio...   \n",
       "5  come this the white house now advising everyone not head the grocery store pharmacy the coming two weeks the next two weeks are extraordinarily important white house response coordinator deborah b...   \n",
       "6                                                   japan got great food mascots amp check out this one spotted month back haneda amp perhaps worth buying you worried about hygiene amp shortages the age   \n",
       "7  earlier announced they are closing select doors now store closures inevitable most non essential retail will close luxury players amp the well capitalized should survive everyone else start your b...   \n",
       "8              tip wash your hands with soap for seconds and use greater than alcohol based hand sanitizer whenever you return home from any activity that involves locations where other people have been   \n",
       "9                                        interesting discovery consumer grade thermal tech won work for medical use cases such detecting covid fevers this the one pro camera action body temp way too low   \n",
       "\n",
       "  Sentiment  IsRelevant     ID  tfVector  \\\n",
       "0   Neutral           0  27369       NaN   \n",
       "1  Positive           1   5035       NaN   \n",
       "2  Negative           0    598       NaN   \n",
       "3  Negative           0  34612       NaN   \n",
       "4  Negative           0  15272       NaN   \n",
       "5  Positive           1  29317       NaN   \n",
       "6  Positive           1  16968       NaN   \n",
       "7  Positive           1   1770       NaN   \n",
       "8  Positive           1  21381       NaN   \n",
       "9  Positive           1   7333       NaN   \n",
       "\n",
       "                                                                                                                                                                                                 Des_token  \\\n",
       "0                                                                           [work, retail, store, would, get, orders, drivers, know, told, work, store, near, store, told, day, day, basis, roughly, item]   \n",
       "1     [day, covid, diary, schools, shut, friday, meanwhile, braved, supermarket, armed, carrier, bags, hand, sanitizer, shelves, pretty, empty, also, saw, elderly, gent, wearing, welding, mask, bizarre]   \n",
       "2  [panic, buying, reaction, coronavirus, meant, drop, donations, foodbanks, also, struggling, buy, supplies, grocery, stores, feeding, america, establishes, covid, response, fund, help, food, banks,...   \n",
       "3                                                                                       [help, wonder, one, third, people, saw, grocery, store, wearing, masks, forced, use, covid, patients, cough, face]   \n",
       "4  [unprecedented, times, calls, unprecedented, measures, caf, india, appeals, support, reach, vulnerable, people, urgent, food, supplies, hygiene, kits, accurate, information, prevention, covid, ave...   \n",
       "5     [come, white, house, advising, everyone, head, grocery, store, pharmacy, coming, two, weeks, next, two, weeks, extraordinarily, important, white, house, response, coordinator, deborah, birx, said]   \n",
       "6                                                    [japan, got, great, food, mascots, amp, check, one, spotted, month, back, haneda, amp, perhaps, worth, buying, worried, hygiene, amp, shortages, age]   \n",
       "7  [earlier, announced, closing, select, doors, store, closures, inevitable, non, essential, retail, close, luxury, players, amp, well, capitalized, survive, everyone, else, start, business, continui...   \n",
       "8                                                          [tip, wash, hands, soap, seconds, use, greater, alcohol, based, hand, sanitizer, whenever, return, home, activity, involves, locations, people]   \n",
       "9                                            [interesting, discovery, consumer, grade, thermal, tech, work, medical, use, cases, detecting, covid, fevers, one, pro, camera, action, body, temp, way, low]   \n",
       "\n",
       "                                                                                                                                                                                                    vector  \\\n",
       "0  [0.5124790327156227, 0.4361002119303823, 0.598646048878666, 0.5733125012590152, 0.6037498071939685, 0.5274970532365403, 0.4721870527564337, 0.5475916829654918, 0.5308112906366967, 0.48899337409897...   \n",
       "1  [0.5704910972802842, 0.42575185715042246, 0.4029838927139648, 0.4468534234028975, 0.5703589785802079, 0.4997081231912942, 0.5690486378334831, 0.5628777879607043, 0.5074086862907985, 0.479480671662...   \n",
       "2  [0.40902034422222444, 0.48297367409893754, 0.5720102381228351, 0.5024366942754132, 0.5595574700839206, 0.48308097911771897, 0.5265511825441006, 0.5386432218716487, 0.5678654419920186, 0.4369796631...   \n",
       "3  [0.5469317265237348, 0.43453320213120455, 0.5551190337066506, 0.5392189638575862, 0.4937792399801246, 0.47348353336298626, 0.6429286203435995, 0.47986935553837523, 0.48194369694910566, 0.479314593...   \n",
       "4  [0.5291587559496079, 0.5223125262482532, 0.5753982354647033, 0.5025361869393413, 0.560602881778188, 0.5467917188106997, 0.5087006598894713, 0.45608143839640897, 0.43031883293045964, 0.445982860158...   \n",
       "5  [0.47007189003721184, 0.4204719778934094, 0.5015609020480467, 0.4885943592785753, 0.44131183004226, 0.5371532611159601, 0.49100569516295794, 0.5622903665518074, 0.4844938677326825, 0.5696832431299...   \n",
       "6  [0.545791345311021, 0.5603386347505851, 0.45267016461493637, 0.5254609942445622, 0.4983894252846796, 0.46931479714409174, 0.4051462448146384, 0.6386083200416551, 0.4442441333783032, 0.333136731118...   \n",
       "7  [0.45208328296077926, 0.5508184592911856, 0.5642579273214066, 0.5295067214475077, 0.5244205173411708, 0.5065622321868268, 0.5324084205359669, 0.5106640908545929, 0.4256287116742515, 0.400963577206...   \n",
       "8  [0.5118022156369622, 0.561365798304358, 0.5359597525348093, 0.5809599342008541, 0.4202774099598013, 0.3551222709885664, 0.46811622067125264, 0.5567428197801534, 0.5660807258123945, 0.3573019305186...   \n",
       "9  [0.5463184304658134, 0.48810772070873637, 0.6176658352086668, 0.5096231623674524, 0.5542031067367708, 0.4519572350732188, 0.3272063781075404, 0.5116562078223029, 0.49214538979283373, 0.57832158894...   \n",
       "\n",
       "   similarity  \n",
       "0    0.918534  \n",
       "1    0.918260  \n",
       "2    0.918055  \n",
       "3    0.917843  \n",
       "4    0.917647  \n",
       "5    0.917431  \n",
       "6    0.917388  \n",
       "7    0.917224  \n",
       "8    0.917160  \n",
       "9    0.916869  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DocumentList['similarity']=DocumentList['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "DocumentList.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "DocumentList.head(10).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f9bc077dbce6d3d5fe29fa6676858a1bcd9a965b73328f2afbecaceda5ad6e3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nltk_pipeline': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
