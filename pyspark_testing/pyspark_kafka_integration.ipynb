{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.6.jar pyspark-shell'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import math\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "#spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_INPUT_TOPIC_NAME_CONS = 'quickstart-events'\n",
    "KAFKA_OUTPUT_TOPIC_NAME_CONS = 'outputmallstream'\n",
    "KAFKA_BOOTSTRAP_SERVERS_CONS = 'localhost:9092'\n",
    "MALL_LONGITUDE=78.446841\n",
    "MALL_LATITUDE=17.427229\n",
    "MALL_THRESHOLD_DISTANCE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName('PySpark Structured Streaming with Kafka') \\\n",
    ".master('local[*]') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = pyspark.SparkConf().setAll([('spark.executor.memory', '2g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','2g')])\n",
    "#spark.stop()\n",
    "#sc = spark.sparkContext\n",
    "#sc.stop()\n",
    "#spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_detail_df = spark.readStream.format('kafka').option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS_CONS).option('subscribe', KAFKA_INPUT_TOPIC_NAME_CONS) \\\n",
    ".option('startingOffsets', 'latest').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_detail_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_detail_df = stream_detail_df.selectExpr('CAST(key AS STRING)','CAST(value AS STRING)', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_detail_df.writeStream \\\n",
    "      .format(\"parquet\") \\\n",
    "      .outputMode(\"append\") \\\n",
    "      .option(\"path\",\"c:/tmp/spark_out/parquet\") \\\n",
    "      .option(\"checkpointLocation\", \"c:/tmp/checkpoint\") \\\n",
    "      .trigger(processingTime='60 seconds')\\\n",
    "      .start() \\\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://cumsum.wordpress.com/2019/02/06/typeerror-invalid-argument-not-a-string-or-column-mysterious-error-when-creating-a-udf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia=SIA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_detection(text):\n",
    "    #text1= tokenizer.tokenize(text)\n",
    "    #\n",
    "    score = sia.polarity_scores(text)\n",
    "    score=score['compound']\n",
    "    if score>=0.05:\n",
    "        sentiment='Positive'\n",
    "    elif score<=(-0.05):\n",
    "        sentiment='Negative'\n",
    "    else:\n",
    "        sentiment='Neutral'\n",
    "        \n",
    "    ##total operation now will be here\n",
    "    #sentiment='maoo'\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classification(words):\n",
    "    sentiment_detection_udf = udf(sentiment_detection, StringType())\n",
    "    words = words.withColumn(\"sentiment\", sentiment_detection_udf(\"value\")) ##column name here will pass \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('PySpark Structured Streaming with Kafka') \\\n",
    "        .master('local[*]') \\\n",
    "        .getOrCreate()\n",
    "    stream_detail_df = spark.readStream.format('kafka').option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS_CONS).option('subscribe', KAFKA_INPUT_TOPIC_NAME_CONS) \\\n",
    "        .option('startingOffsets', 'latest').load()\n",
    "    stream_detail_df = stream_detail_df.selectExpr('CAST(key AS STRING)','CAST(value AS STRING)','timestamp')\n",
    "    \n",
    "    words = text_classification(stream_detail_df)\n",
    "\n",
    "    query=words.writeStream \\\n",
    "      .format(\"parquet\") \\\n",
    "      .outputMode(\"append\") \\\n",
    "      .option(\"path\",\"c:/tmp/spark_out/parquet\") \\\n",
    "      .option(\"checkpointLocation\", \"c:/tmp/checkpoint\") \\\n",
    "      .start() \n",
    "    query.awaitTermination()  ##optional value 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_detail_df = stream_detail_df.selectExpr('CAST(key AS STRING)','CAST(value AS STRING)','timestamp')\n",
    "    \n",
    "words = text_classification(stream_detail_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_detail_write_stream_1 = words.writeStream \\\n",
    ".format('kafka') \\\n",
    ".option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS_CONS) \\\n",
    ".option('topic', KAFKA_OUTPUT_TOPIC_NAME_CONS) \\\n",
    ".trigger(processingTime='60 seconds') \\\n",
    ".outputMode('update') \\\n",
    ".option('checkpointLocation','C:\\pyspark_testing\\checkpoint') \\\n",
    ".start() \\\n",
    ".awaitTermination()  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f9bc077dbce6d3d5fe29fa6676858a1bcd9a965b73328f2afbecaceda5ad6e3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nltk_pipeline': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
